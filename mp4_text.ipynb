{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWFKrpjlKsbT8m5/zvNywU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raghub123/code_repo/blob/main/mp4_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODULE1:\n",
        "Below Module sets the necessary installations and spark imports**"
      ],
      "metadata": {
        "id": "96FjU4YEax5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#necessary installation steps\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "!pip install numpy\n",
        "!pip install keras-ocr\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "NaFtR57TsXyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODULE 2 : Extracts every frame from the input mp4 and stores it in output_images directory and also stores the frame info in frame_data.txt in the format \" timestamp of the frame| frame name**\n",
        "\n",
        "Prerequisites:\n",
        "1. upload the video only in \"/content/drive/My Drive/My_Folder/\" and save it as sample_video.mp4\n"
      ],
      "metadata": {
        "id": "IStco9CebJiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"VideoToImages\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Function to check if two frames are identical\n",
        "def frames_identical(frame1, frame2):\n",
        "    return np.array_equal(frame1, frame2)\n",
        "\n",
        "# Function to convert video to images\n",
        "def video_to_images(video_path, output_folder,frame_file):\n",
        "    # Open the video file\n",
        "    video_capture = cv2.VideoCapture(video_path)\n",
        "    fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total_frames / fps\n",
        "\n",
        "    # Read the first frame\n",
        "    success, frame = video_capture.read()\n",
        "    count = 0\n",
        "\n",
        "    prev_frame = None\n",
        "    # Loop through the video frames\n",
        "    while success:\n",
        "        # Write the frame to an image file\n",
        "        if prev_frame is not None and frames_identical(frame, prev_frame):\n",
        "            # Skip saving duplicate frame\n",
        "            print(f\"Skipping duplicate frame {count}\")\n",
        "        else:\n",
        "            # Convert frame to JPEG format\n",
        "            cv2.imwrite(output_folder + \"/frame%d.jpg\" % count, frame)\n",
        "            frame_time_seconds = count / fps\n",
        "            frame_time_minutes = int(frame_time_seconds // 60)\n",
        "            frame_time_seconds %= 60\n",
        "            frame_time_hours = int(frame_time_minutes // 60)\n",
        "            frame_time_minutes %= 60\n",
        "\n",
        "        # Get frame time in HH:MM:SS format\n",
        "            frame_time = \"{:02d}:{:02d}:{:02d}\".format(frame_time_hours, frame_time_minutes, int(frame_time_seconds))\n",
        "            with open(os.path.join(frame_file), \"a\") as file:\n",
        "               file.write(f\"Frame Time: {frame_time}|\")\n",
        "               file.write(f\"frame%d.jpg\\n\" % count)\n",
        "        prev_frame = frame.copy()\n",
        "\n",
        "        # Read the next frame\n",
        "        success, frame = video_capture.read()\n",
        "        count += 1\n",
        "\n",
        "\n",
        "    video_capture.release()\n",
        "\n",
        "# Define paths\n",
        "video_path = \"/content/drive/My Drive/My_Folder/sample_video.mp4\"\n",
        "output_folder = \"/content/drive/My Drive/My_Folder/output_images\"\n",
        "frame_file =\"/content/drive/My Drive/My_Folder/frame_data.txt\"\n",
        "\n",
        "if os.path.exists(output_folder):\n",
        "   shutil.rmtree(output_folder)\n",
        "   os.makedirs(output_folder)\n",
        "if os.path.exists(frame_file):\n",
        "   os.remove(frame_file)\n",
        "\n",
        "# Convert video to images using Spark\n",
        "spark.sparkContext.parallelize([video_path]).foreach(lambda path: video_to_images(path, output_folder,frame_file))\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "jsc6a2sBBix9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODULE 3: It reads each and every frame present in output_images dir created by module 2 and extratcs the text in each frame and stores in output.txt in the format \"frame name###text\"**\n"
      ],
      "metadata": {
        "id": "MjZQQqlncC_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "# Function to read images from output_images directory\n",
        "def read_images_from_directory(directory):\n",
        "    images = []\n",
        "    file_names = []\n",
        "    for img_name in os.listdir(directory):\n",
        "        img_path = os.path.join(directory, img_name)\n",
        "        img = Image.open(img_path)\n",
        "        images.append(img)\n",
        "        file_names.append(img_name)  # Extract file name\n",
        "    return images,file_names\n",
        "\n",
        "# previous frame extraction module stores frames in below mentioned directory\n",
        "directory = '/content/drive/My Drive/My_Folder/output_images'\n",
        "\n",
        "# Read images from directory\n",
        "images,file_names = read_images_from_directory(directory)\n",
        "\n",
        "def delete_file(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        os.remove(file_path)\n",
        "\n",
        "# Function to perform text recognition using Pytesseract\n",
        "def recognize_text(images, file_names, output_file):\n",
        "    with open(output_file, 'a') as f:\n",
        "        for img, file_name in zip(images, file_names):\n",
        "            # Recognize text using Pytesseract\n",
        "            text = pytesseract.image_to_string(img, lang='eng', config='--psm 6').replace('\\n', '').strip()\n",
        "            # Write filename and text to output file\n",
        "            f.write(f\"{file_name} ### {text}\\n\")\n",
        "            del file_name,text\n",
        "            gc.collect()\n",
        "\n",
        "# Specify the output file path\n",
        "output_file = \"/content/drive/My Drive/My_Folder/output.txt\"\n",
        "\n",
        "# Delete the output file if it exists\n",
        "delete_file(output_file)\n",
        "\n",
        "texts = recognize_text(images, file_names, output_file)\n",
        "\n",
        "\n",
        "# Cleanse the text by removing unwanted characters\n",
        "def cleanse_text(text):\n",
        "    if text is None:\n",
        "        return \"\"  # Return empty string if text is None\n",
        "    else:\n",
        "        # Remove '\\n' and '\\x0c' characters and strip leading/trailing whitespace\n",
        "        return text.replace('\\n', '').replace('\\x0c', '').strip()"
      ],
      "metadata": {
        "id": "yWZoOeFy5MI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODULE 4: It plays the input video when executed and prompts the user to search for the text and fetches the respective frame info and timestamp of the frame at which text is occuring and haults the video at that point**\n",
        "\n",
        "Prerequisite:\n",
        "1. The input video should be uploaded only in google drive with the name \"sample_vide.mp4\" under /content/drive/MyDrive/My_Folder/o\n",
        "2. Get the video Id manually from the google drive and paste in row 19 \"google_drive_video_id \" variable"
      ],
      "metadata": {
        "id": "kKYJFnE3cvz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"VideoPlayer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define a function to open the video link in a new tab with a specific start time\n",
        "def open_video_link(video_id, start_time=None):\n",
        "    if start_time is None:\n",
        "        video_link = f\"https://drive.google.com/file/d/{video_id}/preview\"\n",
        "    else:\n",
        "        video_link = f\"https://drive.google.com/file/d/{video_id}/preview#t={start_time}\"\n",
        "    html_code = f'<iframe src=\"{video_link}\" width=\"640\" height=\"480\"></iframe>'\n",
        "    display(HTML(html_code))\n",
        "\n",
        "# Provide the ID of your video hosted on Google Drive\n",
        "google_drive_video_id = \"1iw0c9KM6kU2PxLB_13YAaTdHnc0uvZ5q\"  # Example ID\n",
        "#https://drive.google.com/file/d/1iw0c9KM6kU2PxLB_13YAaTdHnc0uvZ5q/view?usp=drive_link\n",
        "\n",
        "# Open the video link in a new tab without specifying the start time\n",
        "open_video_link(google_drive_video_id)\n",
        "\n",
        "# Prompt the user to manually start the video and then enter the start time (in seconds)\n",
        "input(\"Manually start the video by clicking the play button, then press Enter to continue...\")\n",
        "\n",
        "\n",
        "# Load the text file as a DataFrame\n",
        "text_file_path = \"/content/drive/MyDrive/My_Folder/output.txt\"  # Replace with the actual path to your file\n",
        "text_df = spark.read.text(text_file_path)\n",
        "\n",
        "# Prompt the user to enter the text to search for\n",
        "search_text = input(\"Enter the text to search for: \").strip().upper()\n",
        "\n",
        "# Define a function to extract frame number and text from each line\n",
        "def extract_frame_text(line):\n",
        "    parts = line.split(\"###\")\n",
        "    if len(parts) == 2:\n",
        "        frame_number, text = parts\n",
        "        return (frame_number.strip(), text.strip())\n",
        "    return (None, None)\n",
        "\n",
        "# Apply the extraction function and filter out None values\n",
        "frame_text_df = text_df.rdd.map(lambda row: extract_frame_text(row.value)).filter(lambda x: x[1] is not None).toDF([\"FrameNumber\", \"Text\"])\n",
        "\n",
        "# Filter the DataFrame to find the first occurrence of the search text\n",
        "matching_row = frame_text_df.filter(frame_text_df[\"Text\"].contains(search_text)).first()\n",
        "\n",
        "if matching_row is None:\n",
        "    print(f\"The text '{search_text}' was not found in the video.\")\n",
        "else:\n",
        "    frame_number = matching_row[\"FrameNumber\"]\n",
        "    print(f\"The text '{search_text}' was found in frame number: {frame_number}\")\n",
        "\n",
        "    # Load the data_frame.txt file as a DataFrame\n",
        "    data_frame_path = \"/content/drive/MyDrive/My_Folder/frame_data.txt\"  # Replace with the actual path to your file\n",
        "    data_frame_df = spark.read.text(data_frame_path)\n",
        "\n",
        "    # Extract frame number from the search result\n",
        "    frame_number_to_search = frame_number.split(\".\")[0]  # Assuming frame_number is in the format \"frameX.jpg\"\n",
        "\n",
        "    # Filter the DataFrame to find the corresponding timestamp\n",
        "    timestamp_row = data_frame_df.filter(data_frame_df[\"value\"].contains(frame_number_to_search)).first()\n",
        "\n",
        "    if timestamp_row is None:\n",
        "        print(f\"Timestamp not found for frame number: {frame_number}\")\n",
        "    else:\n",
        "        timestamp = timestamp_row[\"value\"].split(\"|\")[0].strip().replace('Frame Time:','')\n",
        "        print(timestamp)\n",
        "        # Split the time string into hours, minutes, and seconds\n",
        "        hours, minutes, seconds = map(int, timestamp.split(\":\"))\n",
        "\n",
        "        # Convert hours, minutes, and seconds to seconds and sum them up\n",
        "        total_seconds = hours * 3600 + minutes * 60 + seconds\n",
        "\n",
        "        # Continue playing the video from the user-entered time\n",
        "        open_video_link(google_drive_video_id, total_seconds)\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "mPM7nTbfHZbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}